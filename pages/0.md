
# Generative Models

<p class="text-2xl">

$\mathcal{X}\leftarrow f(\mathcal{Z})$

</p>

<img class="ml-auto mr-auto w-3/4" src="/img/three-generative-models.png" />

<p class="text-center">

Overview of different types of generative models. \[[1](https://lilianweng.github.io/posts/2018-10-13-flow-models/)\]

</p>

---

# Generative Models

<p class="text-2xl">Explicit/Implicit Density Estimation</p>

## Explicit
<p v-click=1 class="text-xl">

e.g. VAE, Flow-based Models  
Estimates the true pdf or cdf over the sample space, **Stable**  

</p>

<p v-click=3 class="text-xl">However, there will be problems such as low generation quality, limited architecture, high computing cost, etc.</p>

## Implicit
<p v-click=2 class="text-xl">Like GAN, it uses confrontation training to approximate the real distribution and has excellent generation quality.</p>
<p v-click=4 class="text-xl">But it is difficult to train stably and prone to mode collapse.</p>

<!--

主流的生成模型可分成透過對抗式學習隱式逼近的 GAN 與直接學習顯示估計的 VAE、Flow-based

GAN 可以生成出高品質的樣本，然而其訓練方式會使收斂不穩定、並且會出現

- GAN利用一個生成網絡和一個判別網絡相互對抗，最小化Jensen-Shannon Divergence，使得生成數據的分佈盡量接近真實數據的分佈。GAN的優點是生成的數據質量高，缺點是訓練不穩定，無法計算數據的概率密度。
- VAE利用一個編碼器和一個解碼器來實現自動編碼器，並在編碼時引入已知的概率分佈，限制編碼的範圍。VAE的優點是訓練穩定，可以計算數據的概率密度，缺點是生成的數據質量低，並且優化目標不是最大似然估計，而是證據下界。
- flow model利用一個可逆的神經網絡來建立訓練數據和生成數據之間的確定性和一對一的關係，最大化似然估計，使得生成數據的分佈與真實數據的分佈完全相同。flow model的優點是理論上最接近問題的本質，可以計算數據的概率密度，缺點是模型設計複雜，計算資源消耗大。

-->

---

# Generation Process

<img src="/img/generative-process.png"/>
<br/>
<img v-click src="/img/generative-process-dpm.png"/>

<!--
使用 MC 將目標分布 X 與已知分布 Z 串聯
在生成過程中就能明確知道每個狀態轉移的機率分布
-->

---

# Why Use Markov Chains

<p class="text-2xl">Connect two distributions</p>

<p class="text-xl">

1. Each state is only related to the previous state.
2. When the diffusion process (forward process) $q(x_t|x_{t-1})$ is **Normal Distribution or Binomial Distribution**, as long as the **variation is small** enough, the reverse process $q(x_{t-1}|x_t)$ will also be **the same type of distribution**.

<!-- In other words, $q(x_{t-1}|x_t)$ can be calculated as long as $\tilde{\mu}(x_t)$ $\tilde{\Sigma}(x_t)$ can be estimated.-->

</p>

<img class="ml-auto mr-auto w-5/6" src="/img/generative-process-dpm-3.png"/>

<p v-click class="text-center test-2xl">

$$q(x_0)=\int q(x_{0..T})\;dx_{1..T}$$

</p>

---

# Variational Lower Bound

<p class=text-2xl>
Derivation of Objectives
</p>

$$
\begin{aligned}
L_\text{CE}
&= - \mathbb{E}_{q(\mathbf{x}_0)} \log p_\theta(\mathbf{x}_0) = - \mathbb{E}_{q(\mathbf{x}_0)} \log \Big( \int p_\theta(\mathbf{x}_{0:T}) d\mathbf{x}_{1:T} \Big) \\
\\
&\leq \mathbb{E}_{q(\mathbf{x}_{0:T})}\Big[\log \frac{q(\mathbf{x}_{1:T} \vert \mathbf{x}_{0})}{p_\theta(\mathbf{x}_{0:T})} \Big] = L_\text{VLB}\\
&= \mathbb{E}_q [\underbrace{D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T))}_{L_T} + \sum_{t=2}^T \underbrace{\color{#a61e4d}{D_\text{KL}(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t))}}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)}_{L_0} ]
\end{aligned}
$$

In fact, the goal of learning is not $q(x_{t-1}|x_t)$, but $q(x_{t-1}|x_t,x_0)$ of $L_t$

By Bayes' rules, it can be rewritten to consist only of forward process  
$q(x_{t-1}|x_t,x_0)=q(\mathbf{x}_t \vert \mathbf{x}_{t-1}, \mathbf{x}_0) \cfrac{ q(\mathbf{x}_{t-1} \vert \mathbf{x}_0) }{ q(\mathbf{x}_t \vert \mathbf{x}_0)}$

---


<img class="ml-auto mr-auto w-2/3" src="/img/forward-backward.png"/>

<p v-click>

$$\begin{aligned}x_t&\sim\mathcal{N}(\sqrt{\alpha_t}x_{t-1},1-\alpha_t\mathbf{I}) &&;\text{let}\;\alpha_i=1-\beta_i\\
&=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon_{t-1} &&;\text{where}\;\epsilon_i\sim\mathcal{N}(0,I)\\ 
&=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{\alpha_{t}}\sqrt{1-\alpha_{t-1}}\epsilon_{t-2}+\sqrt{1-\alpha_t}\epsilon_{t-1}\\
&=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}}\bar{\epsilon}_{t-2}&&;\text{where}\;\bar{\epsilon}_{t-2}\;\text{merges two Gaussians}\\
&=\ldots\\
&=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon\sim\mathcal{N}(\sqrt{\bar{\alpha}_t}x_0,\sqrt{1-\bar{\alpha}_t}\mathbf{I})&&;\bar{\alpha}\_t=\prod^t_{i=1}\alpha_i
\end{aligned}$$

</p>

<!--

簡單來說，forward process 就是不斷加入 gaussian noise
backward process 則是不斷去除 gaussian noise

即使 $\beta_t$ 很小，只要 T 夠多就能使 $q(x_T)=\mathcal{N}(\mathbf{0},\mathbf{I})$

-->

---

# Backward Process

<p class="text-2xl">Using Bayes' rule</p>

$$
\begin{aligned}
q(x_t|x_{t-1})&=\mathcal{N}\Bigl(x_t;\sqrt{\alpha_t}x_{t-1},\beta_t\mathbf{I}\Bigr)
\propto \exp \Big(-\cfrac{(\mathbf{x}_t - \sqrt{\alpha_t} \mathbf{x}_{t-1})^2}{2\beta_t}\Bigr)\\
q(x_t|x_0)&=\mathcal{N}\Bigl(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)\mathbf{I}\Bigr)
\propto \exp \Big(-\cfrac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0)^2}{2(1-\bar{\alpha}_t)}\Bigr)\\
q(x_{t-1}|x_{t},x_0)&=q(x_t|x_{t-1},x_0)\cfrac{q(x_{t-1}|x_0)}{q(x_t|x_0)}\\
&\propto \exp \Big(-\frac{1}{2} \big(\frac{(\mathbf{x}_t - \sqrt{\alpha_t} \mathbf{x}_{t-1})^2}{\beta_t} + \frac{(\mathbf{x}_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0)^2}{1-\bar{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0)^2}{1-\bar{\alpha}_t} \big) \Big) \\
&= \exp\Big( -\frac{1}{2} \big( \textcolor{#a61e4d}{(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}})} \mathbf{x}_{t-1}^2 - \textcolor{#0b7285}{(\frac{2\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{2\sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0)} \mathbf{x}_{t-1} + C(\mathbf{x}_t, \mathbf{x}_0) \big) \Big)\\
&= \exp\Big( -\frac{1}{2} \cfrac{\big(x_{t-1}-\tilde\mu(x_t,x_0)\big)^2}{\tilde{\Sigma}(x_t,x_0)}\Big)
\end{aligned}
$$

---

# Backward Process

<p class="text-2xl">Estimate target</p>

$$
\begin{aligned}
\tilde{\Sigma}(x_t,x_0) 
&= \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t \\
\tilde{\mu} (\mathbf{x}_t, \mathbf{x}_0)
&= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0\\
&= \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon} \Big) &&; x_0=\cfrac{1}{\sqrt{\bar{\alpha}_t}}(x_t - \sqrt{1-\bar{\alpha}_t}\epsilon)
\end{aligned}
$$

<p class="text-2xl text-center">

$$q(x_{t-1}|x_t,x_0)=\textcolor{#a61e4d}{\mathcal{N}(x_{t-1};\cfrac{1}{\sqrt{\bar{\alpha}_t}}(x_t - \sqrt{1-\bar{\alpha}_t}\epsilon),\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t)}$$

</p>

<p v-click class="text-2xl text-center">

**The Goal is to Estimate $x_0$ or $\epsilon$ from $x_t$.**

</p>

---

# DPM Algorithm

<img class="ml-auto mr-auto w-4/5" src="/img/algo.png"/>

<img v-click src="/img/train-and-sample.png"/>
